{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Notebook: 00_CAC40.ipynb\n",
    "# Purpose : Baseline CAC40 time-series research (returns, stationarity, volatility)\n",
    "# =============================================================================\n",
    "\n",
    "# --- Standard Library ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import itertools # Hyperparameter grid searching (windows, lags, etc.)\n",
    "from datetime import datetime, timedelta # Time window management for sliding/walk-forward windows\n",
    "import warnings\n",
    "\n",
    "# --- Data Handling & Math ---\n",
    "import numpy as np # Vectorized operations on returns and signals\n",
    "import pandas as pd # Core TimeSeries container (OHLCV data)\n",
    "import yfinance as yf # Quick ingestion of historical market data\n",
    "\n",
    "# --- Statistics & Diagnostics ---\n",
    "from scipy.stats import jarque_bera, shapiro # Normality tests for return distributions\n",
    "from statsmodels.tsa.stattools import adfuller # Augmented Dickey-Fuller (ADF) for unit root/stationarity\n",
    "from statsmodels.stats.diagnostic import het_arch # Test for ARCH effects (volatility clustering)\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox # Test for remaining autocorrelation in residuals\n",
    "\n",
    "# --- Time Series Econometrics / Volatility Models ---\n",
    "from arch import arch_model # GARCH models for volatility forecasting/regime detection\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # Identifying significant lags for retraining\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt # Plotting equity curves, drawdowns, and price/signal overlays\n",
    "\n",
    "# --- Notebook Runtime Settings ---\n",
    "warnings.filterwarnings(\"ignore\")  # keep notebooks readable; tighten later for production\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Matplotlib defaults (kept minimal; avoid heavy styling for reproducibility)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Data Ingestion & Persistence\n",
    "> This section handles the data acquisition for the **CAC 40 (^FCHI)**. \n",
    "> To ensure reproducibility and speed, we implement a **local caching mechanism**:\n",
    "> 1. We check if a local `.parquet` file exists in the `data/` directory.\n",
    "> 2. If it exists, we load it to avoid unnecessary API calls.\n",
    "> 3. If not, we fetch the data from Yahoo Finance and serialize it locally for future use.\n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data Ingestion & Persistence — CAC 40 (^FCHI)\n",
    "# =============================================================================\n",
    "# - Prefer local Parquet cache for speed and reproducibility\n",
    "# - Fallback to Yahoo Finance only if cache is missing\n",
    "# - Normalize index/columns for clean downstream analysis\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------------\n",
    "ticker = \"^FCHI\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "# Note: notebook is in notebooks/00_analysis/, so ../data -> notebooks/data/\n",
    "cache_dir = Path(\"../data\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cache_file = cache_dir / f\"{ticker.replace('^', '')}_{start_date}_{end_date}.parquet\"\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Load from cache or fetch from Yahoo Finance\n",
    "# -------------------------------------------------------------------------\n",
    "if cache_file.exists():\n",
    "    print(f\"--- Loading data from cache: {cache_file} ---\")\n",
    "    data = pd.read_parquet(cache_file)\n",
    "\n",
    "else:\n",
    "    print(f\"--- Fetching data for {ticker} from Yahoo Finance ---\")\n",
    "\n",
    "    yf_ticker = yf.Ticker(ticker)\n",
    "    data = yf_ticker.history(\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        auto_adjust=False, # keep raw OHLCV (safer for quant work)\n",
    "        repair=True # attempt to fix minor data issues\n",
    "    )\n",
    "\n",
    "    # Basic sanity checks before persisting\n",
    "    if data is None or data.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"No data returned for {ticker} between {start_date} and {end_date}\"\n",
    "        )\n",
    "\n",
    "    # Normalize index and columns for downstream consistency\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data = data.sort_index()\n",
    "\n",
    "    # Keep only OHLCV if present; drop Yahoo extras (Dividends, Stock Splits, etc.)\n",
    "    keep_cols = [c for c in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"] if c in data.columns]\n",
    "    data = data[keep_cols]\n",
    "\n",
    "    # Persist to Parquet for future runs\n",
    "    data.to_parquet(cache_file)\n",
    "    print(f\"--- Data saved to: {cache_file} ---\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Minimal validation\n",
    "# -------------------------------------------------------------------------\n",
    "required = {\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"}\n",
    "missing = required - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required OHLC columns: {missing}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Preview\n",
    "# -------------------------------------------------------------------------\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index.min()} → {data.index.max()}\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Markdown Cell: Exploratory Data Analysis - Visual Inspection\n",
    "\n",
    "> Before applying statistical tests, we perform a **Visual Inspection** of the time series. This step is crucial to identify:\n",
    "> 1. **Trend**: Does the series move around a constant mean (stationary) or does it exhibit a long-term direction (non-stationary)?\n",
    "> 2. **Seasonality**: Are there recurring patterns at fixed intervals?\n",
    "> 3. **Variance**: Does the fluctuation amplitude change over time (**Heteroskedasticity**)?\n",
    "> 4. **Structural Breaks**: Major market shocks that might require a model reset.\n",
    "> \n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Exploratory Data Analysis — Visual Inspection (Level 1)\n",
    "# =============================================================================\n",
    "# Objectives:\n",
    "# 1) Detect long-term trend (stationarity vs. drift)\n",
    "# 2) Identify potential structural breaks or crisis periods\n",
    "# 3) Assess volatility clustering (heteroskedasticity)\n",
    "# =============================================================================\n",
    "\n",
    "# Prefer Adj Close if available, otherwise fallback to Close\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in data.columns else \"Close\"\n",
    "cac40 = data[price_col].loc[:end_date]\n",
    "\n",
    "# Quick sanity preview\n",
    "cac40.head()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 1 — Raw price series (linear scale)\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.plot(cac40.index, cac40, linewidth=1.2, label=\"CAC 40\")\n",
    "\n",
    "# Add a long-term smoothing to visualize trend (200-day rolling mean)\n",
    "cac40.rolling(window=200, min_periods=50).mean().plot(\n",
    "    linewidth=2.0, label=\"200-day rolling mean\"\n",
    ")\n",
    "\n",
    "plt.title(\"CAC 40 Index — Visual Inspection (2000–2025)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(f\"{price_col}\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 2 — Log scale (better for volatility / multiplicative dynamics)\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.plot(cac40.index, cac40, linewidth=1.2)\n",
    "plt.yscale(\"log\")  # Important for financial time series\n",
    "\n",
    "plt.title(\"CAC 40 Index — Log Scale (volatility & regime changes)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(f\"{price_col} (log scale)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### From Prices to Returns: Why We Change Perspective\n",
    "\n",
    "The visual inspection of the **CAC 40 price level** suggests that the series is **not stationary**:  \n",
    "it exhibits long-term trends, regime shifts, and large swings in magnitude. This is expected for\n",
    "financial price series and makes many statistical tests (e.g., ADF, ARCH, ARMA) invalid **on prices directly**.\n",
    "\n",
    "To address this, we analyze **returns instead of prices**. Returns represent *relative* changes in value and\n",
    "typically:\n",
    "\n",
    "- Remove most deterministic trends,\n",
    "- Stabilize the variance over time,\n",
    "- Make the series more amenable to stationarity tests,\n",
    "- Better reflect the quantity that investors actually care about (gains and losses).\n",
    "\n",
    "We focus on **log returns**, defined as:\n",
    "\n",
    "$$\n",
    "r_t = \\ln(P_t) - \\ln(P_{t-1})\n",
    "$$\n",
    "\n",
    "Log returns are preferred in quantitative finance because they are:\n",
    "- Time-additive,\n",
    "- Symmetric for gains and losses,\n",
    "- Consistent with continuous-time models used in econometrics and risk management.\n",
    "\n",
    "In the next cell, we compute and visually inspect the daily log returns of the CAC 40.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Returns Computation & Visual Inspection\n",
    "# =============================================================================\n",
    "# Compute daily log returns and inspect their dynamics visually\n",
    "# =============================================================================\n",
    "\n",
    "# Select price series (robust fallback)\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in data.columns else \"Close\"\n",
    "prices = data[price_col].loc[:end_date]\n",
    "\n",
    "# Compute daily log returns\n",
    "returns = np.log(prices / prices.shift(1)).dropna()\n",
    "returns.name = \"CAC40 Log Returns\"\n",
    "\n",
    "# Quick sanity check\n",
    "print(f\"Returns shape: {returns.shape}\")\n",
    "print(f\"Date range: {returns.index.min()} → {returns.index.max()}\")\n",
    "returns.head()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 1 — Time series of log returns (stationarity visual check)\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.plot(returns.index, returns, linewidth=0.8)\n",
    "plt.axhline(0, color=\"black\", linewidth=1.0, linestyle=\"--\")\n",
    "\n",
    "plt.title(\"CAC 40 Daily Log Returns (2000–2025)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 2 — Rolling volatility (visual check for heteroskedasticity)\n",
    "# -----------------------------------------------------------------------------\n",
    "rolling_vol = returns.rolling(window=21, min_periods=10).std() * np.sqrt(252)\n",
    "\n",
    "plt.plot(rolling_vol.index, rolling_vol, linewidth=1.2)\n",
    "plt.title(\"CAC 40 — Rolling Annualized Volatility (21-day window)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Annualized Volatility\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### From Visual Intuition to Formal Time-Series Diagnostics\n",
    "\n",
    "Our preliminary **visual inspection** led to two key empirical observations:\n",
    "\n",
    "1. **Price levels are non-stationary**  \n",
    "   The CAC 40 index exhibits clear long-term trends, structural breaks, and regime shifts.  \n",
    "   This behavior is typical of financial prices and suggests that statistical inference on\n",
    "   the raw price series is likely to be unreliable without transformation.\n",
    "\n",
    "2. **Returns appear much closer to stationarity**  \n",
    "   After transforming prices into log returns, the series fluctuates around a stable mean\n",
    "   with no obvious deterministic trend. This is consistent with standard assumptions in\n",
    "   financial econometrics.\n",
    "\n",
    "3. **Volatility clustering is visually evident**  \n",
    "   Periods of large fluctuations in returns tend to be followed by further large fluctuations,\n",
    "   while calmer periods tend to persist — a hallmark of conditional heteroskedasticity\n",
    "   (ARCH/GARCH behavior).\n",
    "\n",
    "---\n",
    "\n",
    "### Next Step: Autocorrelation Analysis on Prices\n",
    "\n",
    "Before formally testing stationarity and modeling volatility, we first examine the **autocorrelation structure of the price series** using:\n",
    "\n",
    "- The **Autocorrelation Function (ACF)** — to assess persistence and long-memory behavior,\n",
    "- The **Partial Autocorrelation Function (PACF)** — to identify the potential order of an AR process.\n",
    "\n",
    "If prices are truly non-stationary (e.g., integrated of order 1), we expect:\n",
    "- A slowly decaying ACF,\n",
    "- Significant autocorrelations at many lags,\n",
    "- A PACF that does not cut off sharply.\n",
    "\n",
    "We now compute and visualize the ACF and PACF of the CAC 40 price level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Autocorrelation Analysis — Price Level (CAC 40)\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Confirm visually suspected non-stationarity of the price series\n",
    "# - Inspect persistence structure via ACF and PACF\n",
    "# =============================================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# ACF: measures correlation of the series with its own past values\n",
    "# -------------------------------------------------------------------------\n",
    "# For a non-stationary (integrated) series, we typically expect:\n",
    "# - Slow decay of autocorrelations\n",
    "# - Many significant lags\n",
    "plot_acf(cac40, lags=50, ax=ax1)\n",
    "ax1.set_title(\"ACF of CAC 40 Index (2000–2025)\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PACF: controls for intermediate lags, useful for AR order identification\n",
    "# -------------------------------------------------------------------------\n",
    "# For a stationary AR(p) process, PACF should cut off after lag p.\n",
    "# For non-stationary series, PACF often remains significant at many lags.\n",
    "plot_pacf(cac40, lags=50, ax=ax2)\n",
    "ax2.set_title(\"PACF of CAC 40 Index (2000–2025)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Autocorrelation Analysis on Returns: From Non-Stationary Prices to Stationary Dynamics\n",
    "\n",
    "The ACF of the **price level** confirmed our earlier visual intuition: the slow decay of\n",
    "autocorrelations is consistent with a non-stationary (near unit-root) process.\n",
    "\n",
    "We now shift our focus to **log returns**, which are expected to behave very differently.\n",
    "\n",
    "Our objectives in this step are to examine whether:\n",
    "\n",
    "1. **Linear dependence is largely absent in returns**  \n",
    "   If returns are (approximately) weakly stationary, we should observe:\n",
    "   - ACF values close to zero beyond very short lags,\n",
    "   - A PACF that cuts off quickly, indicating little predictable linear structure.\n",
    "\n",
    "2. **Volatility dependence remains present**  \n",
    "   Even if returns show little linear autocorrelation, this does **not** imply independence.\n",
    "   Financial returns often display:\n",
    "   - No significant autocorrelation in levels,\n",
    "   - But strong dependence in squared or absolute returns (volatility clustering).\n",
    "\n",
    "This distinction is crucial:\n",
    "- ACF/PACF on returns help assess **predictability of the mean**,  \n",
    "- But they do **not** capture volatility dynamics — which we will test later using ARCH diagnostics.\n",
    "\n",
    "We now compute the ACF and PACF of the daily log returns to verify these expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Autocorrelation Analysis — Log Returns (CAC 40)\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Verify whether returns exhibit significant linear autocorrelation\n",
    "# - Contrast with the highly persistent structure observed in prices\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(10, 8))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# ACF of returns: tests for linear dependence in the mean\n",
    "# -------------------------------------------------------------------------\n",
    "# For a (weakly) stationary return series, we typically expect:\n",
    "# - Most autocorrelations within confidence bounds after very short lags\n",
    "# - No long-range persistence as seen in price levels\n",
    "plot_acf(returns, ax=ax[0], lags=20, alpha=0.05)\n",
    "ax[0].set_title(\"ACF of CAC 40 Daily Log Returns (2000–2025)\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PACF of returns: useful for detecting short AR structures\n",
    "# -------------------------------------------------------------------------\n",
    "# If returns were well-described by a low-order AR(p) process,\n",
    "# the PACF would cut off sharply after lag p.\n",
    "# In practice, financial returns usually show little such structure.\n",
    "plot_pacf(returns, ax=ax[1], lags=20, alpha=0.05)\n",
    "ax[1].set_title(\"PACF of CAC 40 Daily Log Returns (2000–2025)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Formal Stationarity Test on Returns (Augmented Dickey–Fuller)\n",
    "\n",
    "The ACF and PACF of the **log returns** suggested an absence of linear autocorrelation:\n",
    "the series behaves similarly to white noise in its mean dynamics. This implies that\n",
    "predicting *expected returns* with a simple linear time-series model would likely be ineffective.\n",
    "\n",
    "However, we previously observed clear **volatility clustering**, indicating that while\n",
    "returns may be unpredictable in mean, their **variance is not constant over time**.\n",
    "This motivates the use of a **GARCH-type model** for volatility prediction.\n",
    "\n",
    "Before estimating such a model, we first need to formally verify that the return series\n",
    "is (weakly) stationary. For this purpose, we apply the **Augmented Dickey–Fuller (ADF) test**.\n",
    "\n",
    "The ADF test evaluates the null hypothesis:\n",
    "\n",
    "- **H₀ (non-stationary):** The series has a unit root.\n",
    "- **H₁ (stationary):** The series is stationary.\n",
    "\n",
    "If the test statistic is more negative than the critical values, and the p-value is below\n",
    "a conventional threshold (e.g., 5%), we reject H₀ and conclude that the series is stationary.\n",
    "\n",
    "We now perform the ADF test on daily log returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stationarity Test — Augmented Dickey–Fuller (ADF) on Returns\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Statistically confirm whether log returns are stationary\n",
    "# - Provide formal justification before fitting a GARCH model\n",
    "# =============================================================================\n",
    "\n",
    "# Run ADF test\n",
    "result = adfuller(returns)\n",
    "\n",
    "# Unpack and report key statistics\n",
    "adf_stat = result[0]\n",
    "p_value = result[1]\n",
    "crit_1 = result[4][\"1%\"]\n",
    "crit_5 = result[4][\"5%\"]\n",
    "crit_10 = result[4][\"10%\"]\n",
    "\n",
    "print(\"ADF test statistic:\", adf_stat)\n",
    "print(\"Critical value 1%:\", crit_1)\n",
    "print(\"Critical value 5%:\", crit_5)\n",
    "print(\"Critical value 10%:\", crit_10)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Decision rule (5% significance level)\n",
    "is_stationary = p_value < 0.05\n",
    "print(\"Is stationary?\", \"Yes\" if is_stationary else \"No\")\n",
    "\n",
    "# Optional: explicit interpretation in code (for clarity in reports/notebooks)\n",
    "if is_stationary:\n",
    "    print(\n",
    "        \"Conclusion: We reject the null hypothesis of a unit root.\\n\"\n",
    "        \"Log returns can be treated as (weakly) stationary.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Conclusion: We fail to reject the null hypothesis of a unit root.\\n\"\n",
    "        \"Further transformations or detrending may be required.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Diagnosing Conditional Heteroskedasticity: Squared Returns\n",
    "\n",
    "Having established that **log returns are (weakly) stationary**, we now turn to the dynamics\n",
    "of **volatility** rather than the mean.\n",
    "\n",
    "A key empirical regularity in financial time series is that:\n",
    "- Returns themselves often show little linear autocorrelation,\n",
    "- But their **variance is strongly dependent over time** (volatility clustering).\n",
    "\n",
    "To investigate this, we analyze the **squared returns** \\( r_t^2 \\), which serve as a proxy\n",
    "for realized volatility at each point in time.\n",
    "\n",
    "Our objectives in this step are to examine whether:\n",
    "\n",
    "1. **Squared returns exhibit persistence over time**  \n",
    "   A slowly evolving pattern in \\( r_t^2 \\) would indicate volatility regimes (high-volatility\n",
    "   and low-volatility periods).\n",
    "\n",
    "2. **Autocorrelation is present in squared returns**  \n",
    "   Significant ACF/PACF in \\( r_t^2 \\) suggests that past volatility contains information\n",
    "   about future volatility — a necessary condition for ARCH/GARCH modeling.\n",
    "\n",
    "If these features are present, they provide strong empirical support for specifying\n",
    "a **GARCH-type model** to forecast conditional volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Volatility Diagnostics — Squared Returns\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Visually inspect volatility clustering\n",
    "# - Examine autocorrelation structure of squared returns\n",
    "# =============================================================================\n",
    "\n",
    "# Compute squared returns (proxy for realized volatility)\n",
    "returns_squared = returns**2\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 1 — Time series of squared returns (volatility evolution)\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "returns_squared.plot()\n",
    "plt.title(\"Evolution of CAC 40 Squared Returns (2000–2025)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Squared Returns\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 2 — ACF / PACF of squared returns\n",
    "# -----------------------------------------------------------------------------\n",
    "# Significant autocorrelations here indicate conditional heteroskedasticity,\n",
    "# i.e., today's volatility depends on past volatility (ARCH effect).\n",
    "# -----------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(10, 8))\n",
    "\n",
    "# ACF: persistence of volatility over time\n",
    "plot_acf(returns_squared, ax=ax[0], lags=20, alpha=0.05)\n",
    "ax[0].set_title(\"ACF of CAC 40 Squared Returns (2000–2025)\")\n",
    "\n",
    "# PACF: helps identify potential ARCH/GARCH orders\n",
    "plot_pacf(returns_squared, ax=ax[1], lags=20, alpha=0.05)\n",
    "ax[1].set_title(\"PACF of CAC 40 Squared Returns (2000–2025)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Volatility Modeling with GARCH: Model Selection via AIC\n",
    "\n",
    "Our diagnostics support the following conclusions:\n",
    "\n",
    "- The **price level** is non-stationary (trend / slow ACF decay).\n",
    "- **Returns** are much closer to a weakly stationary process and show little linear autocorrelation\n",
    "  (ACF/PACF consistent with a near white-noise mean).\n",
    "- **Volatility is not constant over time**: squared returns exhibit persistence and significant\n",
    "  autocorrelation, which is consistent with **conditional heteroskedasticity**.\n",
    "\n",
    "This motivates the use of **ARCH/GARCH** models to describe and forecast the **conditional variance**\n",
    "of returns.\n",
    "\n",
    "We now perform a simple model selection exercise for **GARCH(p, q)** by fitting a grid of candidate\n",
    "models and comparing them using the **Akaike Information Criterion (AIC)**:\n",
    "\n",
    "- Lower AIC indicates a better trade-off between goodness of fit and model complexity.\n",
    "- We will restrict ourselves to small orders (p, q in {1,2,3}) for stability and interpretability.\n",
    "\n",
    "Because financial returns often exhibit fat tails, we also consider using a **Student-t**\n",
    "innovation distribution, which is commonly more realistic than Gaussian residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GARCH(p, q) Grid Search — Model Selection via AIC\n",
    "# =============================================================================\n",
    "# Notes:\n",
    "# - Use returns in percent for numerical stability\n",
    "# - Assume zero mean (consistent with ACF/PACF suggesting no linear predictability)\n",
    "# - Use Student-t innovations (fat tails typical in equity index returns)\n",
    "# - Handle convergence failures gracefully\n",
    "# =============================================================================\n",
    "\n",
    "# Scale returns for numerical stability (e.g., 0.01 -> 1%)\n",
    "r = (returns * 100).dropna()\n",
    "\n",
    "p_range = range(1, 4)\n",
    "q_range = range(1, 4)  # q=0 would be ARCH(p); keep q>=1 for true GARCH\n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        try:\n",
    "            model = arch_model(\n",
    "                r,\n",
    "                mean=\"Zero\",      # consistent with \"white noise in mean\"\n",
    "                vol=\"GARCH\",\n",
    "                p=p,\n",
    "                q=q,\n",
    "                dist=\"t\"          # more realistic than normal for returns\n",
    "            )\n",
    "\n",
    "            res = model.fit(disp=\"off\")\n",
    "            rows.append({\n",
    "                \"p\": p,\n",
    "                \"q\": q,\n",
    "                \"aic\": res.aic,\n",
    "                \"bic\": res.bic,\n",
    "                \"loglik\": res.loglikelihood,\n",
    "                \"converged\": bool(getattr(res, \"convergence_flag\", 0) == 0),\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Record failures so the grid search remains transparent\n",
    "            rows.append({\n",
    "                \"p\": p,\n",
    "                \"q\": q,\n",
    "                \"aic\": np.nan,\n",
    "                \"bic\": np.nan,\n",
    "                \"loglik\": np.nan,\n",
    "                \"converged\": False,\n",
    "                \"error\": str(e)[:120],  # keep it short for display\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(by=\"aic\", ascending=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### GARCH Model Selection and Interpretation\n",
    "\n",
    "We estimated a grid of **GARCH(p, q)** models with small orders (p, q ∈ {1, 2, 3}) and\n",
    "compared them using the **Akaike Information Criterion (AIC)**.\n",
    "\n",
    "The main results are summarized below:\n",
    "\n",
    "- Several models provide very similar AIC values, indicating that volatility dynamics\n",
    "  are well captured even with relatively low-order specifications.\n",
    "- The lowest AIC is obtained for **GARCH(2,1)**.\n",
    "- More complex models (e.g., GARCH(3,3)) do not provide a meaningful improvement in fit\n",
    "  despite additional parameters.\n",
    "\n",
    "This behavior is typical in financial time series:\n",
    "- Volatility is highly persistent,\n",
    "- But increasing model complexity beyond a certain point yields diminishing returns.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Choice\n",
    "\n",
    "Although **GARCH(2,1)** achieves the minimum AIC, the difference with **GARCH(1,1)**\n",
    "is relatively small.\n",
    "\n",
    "In practice, model selection should balance:\n",
    "- Statistical fit (AIC/BIC),\n",
    "- Parsimony,\n",
    "- Interpretability,\n",
    "- Stability and robustness of estimation.\n",
    "\n",
    "For these reasons, we retain **GARCH(1,1)** as a baseline volatility model:\n",
    "- It is widely used in empirical finance,\n",
    "- It captures volatility clustering effectively,\n",
    "- It is easier to interpret and forecast,\n",
    "- It is less prone to overfitting.\n",
    "\n",
    "The selected model will now be estimated and analyzed in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Estimation of the GARCH(1,1) Model\n",
    "\n",
    "Based on the previous diagnostics, we retained a **GARCH(1,1)** specification to model\n",
    "the conditional volatility of CAC 40 returns.\n",
    "\n",
    "The model is defined as:\n",
    "\n",
    "\\[\n",
    "r_t = \\varepsilon_t, \\quad \\varepsilon_t = \\sigma_t z_t\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( \\omega \\) is the long-run variance component,\n",
    "- \\( \\alpha \\) captures the immediate impact of shocks (news effect),\n",
    "- \\( \\beta \\) measures volatility persistence,\n",
    "- \\( z_t \\) follows a standardized Student-t distribution to account for fat tails.\n",
    "\n",
    "The model is estimated by **maximum likelihood** under the assumption of zero mean,\n",
    "which is consistent with the absence of linear autocorrelation in returns.\n",
    "\n",
    "We now analyze the estimated parameters and their implications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = arch_model(r, mean='Zero', vol='Garch', p=1, q=1, dist='t')\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Statistical Significance of GARCH Parameters\n",
    "\n",
    "The significance of estimated parameters is assessed using three standard criteria:\n",
    "\n",
    "1. **p-values (P>|t|)**  \n",
    "   A parameter is considered statistically significant if its p-value is below a chosen\n",
    "   threshold (typically 5%).\n",
    "\n",
    "2. **t-statistics**  \n",
    "   A common rule of thumb:\n",
    "   - |t| > 1.96 → significant at 5%\n",
    "   - |t| > 2.58 → significant at 1%\n",
    "\n",
    "3. **Confidence intervals**  \n",
    "   If the 95% confidence interval does not contain 0, the parameter is statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "In the estimated GARCH(1,1) model:\n",
    "\n",
    "- All parameters (ω, α, β, ν) have extremely low p-values,\n",
    "- Very large t-statistics,\n",
    "- Confidence intervals that exclude 0,\n",
    "\n",
    "This implies that **all model parameters are statistically significant** and contribute\n",
    "meaningfully to the dynamics of conditional volatility.\n",
    "\n",
    "The model is therefore both **statistically valid** and **econometrically interpretable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Model Validation: Residual Diagnostics\n",
    "\n",
    "After estimating the GARCH(1,1) model, we must verify that it has successfully\n",
    "captured the dependence structure in the data.\n",
    "\n",
    "A well-specified GARCH model should produce **standardized residuals** that behave\n",
    "like an independent and identically distributed (i.i.d.) process.\n",
    "\n",
    "In practice, this implies the following diagnostic requirements:\n",
    "\n",
    "1. **No linear autocorrelation**  \n",
    "   Residuals should resemble white noise in their mean dynamics.\n",
    "\n",
    "2. **Homoskedasticity of residuals**  \n",
    "   Conditional heteroskedasticity should be absorbed by the model, leaving no\n",
    "   volatility clustering in the residuals.\n",
    "\n",
    "3. **Absence of remaining ARCH effects**  \n",
    "   Squared residuals should not exhibit significant autocorrelation.\n",
    "\n",
    "4. **Distributional adequacy**  \n",
    "   Residuals should be approximately consistent with the assumed innovation\n",
    "   distribution (here: Student-t), acknowledging that perfect normality is not expected\n",
    "   in large financial samples.\n",
    "\n",
    "We begin with **visual diagnostics** and basic statistical tests before proceeding\n",
    "to formal ARCH tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model Validation — Residual Diagnostics (GARCH)\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Check whether standardized residuals behave like white noise\n",
    "# - Assess remaining autocorrelation and distributional properties\n",
    "# =============================================================================\n",
    "\n",
    "# Use standardized residuals (key point!)\n",
    "std_resid = results.std_resid\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 1 — Standardized residuals over time\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(std_resid)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Standardized Residuals from GARCH(1,1)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Standardized Residual\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 2 — ACF / PACF of standardized residuals\n",
    "# -----------------------------------------------------------------------------\n",
    "# Expectation:\n",
    "# - No significant autocorrelation if the mean dynamics are well specified\n",
    "plot_acf(std_resid, lags=20)\n",
    "plt.title(\"ACF of Standardized Residuals\")\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(std_resid, lags=20)\n",
    "plt.title(\"PACF of Standardized Residuals\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Normality test (Shapiro–Wilk)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Note: In large samples, this test almost always rejects normality.\n",
    "# It is used here as a diagnostic, not as a strict acceptance criterion.\n",
    "stat, p = shapiro(std_resid)\n",
    "\n",
    "print(f\"Shapiro-Wilk statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Residual Diagnostics — Synthesis\n",
    "\n",
    "The validation of the GARCH(1,1) model is based on the analysis of **standardized residuals**,\n",
    "which should behave as an independent and identically distributed (i.i.d.) process if the\n",
    "model is correctly specified.\n",
    "\n",
    "The diagnostics lead to the following conclusions:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Time Series of Standardized Residuals\n",
    "\n",
    "The standardized residuals:\n",
    "- Are centered around zero,\n",
    "- Exhibit no visible clustering or persistent structure,\n",
    "- Show large shocks only during extreme market events.\n",
    "\n",
    "This suggests that the conditional variance dynamics have been successfully captured\n",
    "by the GARCH model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. ACF and PACF of Standardized Residuals\n",
    "\n",
    "Both the ACF and PACF show:\n",
    "- No significant autocorrelation beyond lag 0,\n",
    "- All lags lying within the confidence bands.\n",
    "\n",
    "This indicates:\n",
    "- No remaining linear dependence in the residuals,\n",
    "- The mean dynamics are adequately specified (white noise behavior).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Normality Test (Shapiro–Wilk)\n",
    "\n",
    "The Shapiro–Wilk test strongly rejects the null hypothesis of normality.\n",
    "\n",
    "However, this result is:\n",
    "- Expected for financial return data,\n",
    "- Largely driven by the large sample size,\n",
    "- Not problematic in this context, since the model assumes a **Student-t distribution**\n",
    "  for the innovations.\n",
    "\n",
    "---\n",
    "\n",
    "### Interim Conclusion\n",
    "\n",
    "The standardized residuals behave approximately as white noise.\n",
    "This indicates that the **GARCH(1,1) model successfully captures both the mean and variance\n",
    "dynamics of CAC 40 returns**.\n",
    "\n",
    "We now proceed to verify that **no conditional heteroskedasticity remains** in the residuals\n",
    "by analyzing the autocorrelation structure of their squared values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Residual Volatility Diagnostics — Squared Standardized Residuals\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Check for remaining conditional heteroskedasticity\n",
    "# - Validate that GARCH has absorbed volatility clustering\n",
    "# =============================================================================\n",
    "\n",
    "std_resid = results.std_resid\n",
    "std_resid_squared = std_resid**2\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ACF / PACF of squared standardized residuals\n",
    "# -----------------------------------------------------------------------------\n",
    "# Expectation:\n",
    "# - No significant autocorrelation if the GARCH model is well specified\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "plot_acf(std_resid_squared, lags=20)\n",
    "plt.title(\"ACF of Squared Standardized Residuals\")\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(std_resid_squared, lags=20)\n",
    "plt.title(\"PACF of Squared Standardized Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Residual Volatility Diagnostics — Interpretation\n",
    "\n",
    "The analysis of the **squared standardized residuals** provides the final validation step\n",
    "for the GARCH(1,1) model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. ACF and PACF of Squared Standardized Residuals\n",
    "\n",
    "Both the ACF and PACF show:\n",
    "- No significant autocorrelation beyond lag 0,\n",
    "- All coefficients lying within the confidence bounds,\n",
    "- No slow decay or persistent structure.\n",
    "\n",
    "This indicates that:\n",
    "- Volatility clustering has been successfully absorbed by the GARCH model,\n",
    "- There is no remaining conditional heteroskedasticity in the residuals.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Implication for Model Adequacy\n",
    "\n",
    "The absence of autocorrelation in the squared standardized residuals strongly suggests that\n",
    "the **conditional variance is correctly specified**.\n",
    "\n",
    "In other words:\n",
    "- The GARCH(1,1) model has captured the full volatility dynamics present in the data,\n",
    "- No higher-order ARCH/GARCH terms appear necessary.\n",
    "\n",
    "We now proceed to formalize these conclusions using **Ljung–Box tests** on both the\n",
    "standardized residuals and their squared values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Formal Residual Diagnostics — Ljung–Box Tests\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Test for remaining autocorrelation in standardized residuals\n",
    "# - Test for remaining ARCH effects via squared standardized residuals\n",
    "# =============================================================================\n",
    "\n",
    "# Define lags to test (common choice: up to 12 for daily data)\n",
    "lags = list(range(1, 13))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ljung–Box test on standardized residuals\n",
    "# -----------------------------------------------------------------------------\n",
    "# H0: No autocorrelation up to the specified lag\n",
    "lb_resid = acorr_ljungbox(std_resid, lags=lags, return_df=True)\n",
    "\n",
    "print(\"Ljung–Box Test on Standardized Residuals\")\n",
    "display(lb_resid)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ljung–Box test on squared standardized residuals\n",
    "# -----------------------------------------------------------------------------\n",
    "# H0: No autocorrelation in squared residuals (no remaining ARCH effects)\n",
    "lb_resid_sq = acorr_ljungbox(std_resid_squared, lags=lags, return_df=True)\n",
    "\n",
    "print(\"Ljung–Box Test on Squared Standardized Residuals\")\n",
    "display(lb_resid_sq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Ljung–Box Tests — Final Interpretation\n",
    "\n",
    "The Ljung–Box test applied to the **standardized residuals** fails to reject the null\n",
    "hypothesis of no autocorrelation at all tested lags. This confirms that the conditional\n",
    "mean dynamics are correctly specified and that residuals behave as white noise.\n",
    "\n",
    "When applied to the **squared standardized residuals**, the test reveals a more nuanced\n",
    "picture:\n",
    "\n",
    "- For the first two lags, no evidence of remaining ARCH effects is detected.\n",
    "- Mild significance appears at short lags (3–6), with p-values slightly below the 5%\n",
    "  threshold.\n",
    "- Beyond these lags, p-values quickly increase and remain above conventional significance\n",
    "  levels.\n",
    "\n",
    "This suggests the presence of **weak short-term residual heteroskedasticity**, but no\n",
    "persistent or long-memory volatility effects.\n",
    "\n",
    "Overall, the GARCH(1,1) model captures the dominant volatility dynamics of CAC 40 returns,\n",
    "including volatility clustering and persistence. The remaining effects are limited in\n",
    "magnitude and do not indicate major model misspecification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = arch_model(r, mean='Zero', vol='Garch', p=2, q=1, dist='t')\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "std_resid = results.std_resid\n",
    "std_resid_squared = std_resid**2\n",
    "\n",
    "lags = list(range(1, 13))\n",
    "lb_resid = acorr_ljungbox(std_resid, lags=lags, return_df=True)\n",
    "print(\"Ljung–Box Test on Standardized Residuals\")\n",
    "display(lb_resid)\n",
    "\n",
    "lb_resid_sq = acorr_ljungbox(std_resid_squared, lags=lags, return_df=True)\n",
    "print(\"Ljung–Box Test on Squared Standardized Residuals\")\n",
    "display(lb_resid_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Model Refinement: GARCH(2,1) vs GARCH(1,1)\n",
    "\n",
    "Following the diagnostics of the GARCH(1,1) model, mild short-term dependence remained\n",
    "in the squared standardized residuals at very low lags.\n",
    "\n",
    "To address this, a GARCH(2,1) specification was estimated.\n",
    "\n",
    "The results show that:\n",
    "- All additional parameters are statistically significant,\n",
    "- The log-likelihood and AIC are slightly improved,\n",
    "- Ljung–Box tests on squared standardized residuals no longer reject the null hypothesis\n",
    "  at any tested lag.\n",
    "\n",
    "This indicates that the GARCH(2,1) model fully captures the remaining short-term volatility\n",
    "dependence that was not absorbed by the GARCH(1,1) specification.\n",
    "\n",
    "Both models provide a satisfactory representation of CAC 40 volatility dynamics.\n",
    "The GARCH(2,1) offers a marginally better in-sample fit at the cost of one additional\n",
    "ARCH parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### ARCH LM Test (Engle): Motivation and Expected Outcomes\n",
    "\n",
    "Although visual inspection and autocorrelation analysis of the squared standardized\n",
    "residuals suggest that the GARCH model has adequately captured volatility clustering,\n",
    "a **formal statistical test** is required to confirm this result.\n",
    "\n",
    "The **ARCH Lagrange Multiplier (LM) test**, introduced by Engle (1982), is specifically\n",
    "designed to detect the presence of **conditional heteroskedasticity** in a time series.\n",
    "\n",
    "---\n",
    "\n",
    "#### Test Objective\n",
    "\n",
    "The ARCH LM test evaluates whether the variance of the residuals depends on their past\n",
    "values. In the context of a fitted GARCH model, it is used to verify whether **any ARCH\n",
    "effects remain** after accounting for conditional volatility.\n",
    "\n",
    "---\n",
    "\n",
    "#### Hypotheses\n",
    "\n",
    "The test is based on the following hypotheses:\n",
    "\n",
    "- **Null hypothesis (H₀):**  \n",
    "  There are no remaining ARCH effects in the residuals  \n",
    "  (i.e., the conditional variance is constant after GARCH filtering).\n",
    "\n",
    "- **Alternative hypothesis (H₁):**  \n",
    "  The residuals exhibit ARCH effects  \n",
    "  (i.e., conditional heteroskedasticity remains).\n",
    "\n",
    "---\n",
    "\n",
    "#### Expected Result for a Well-Specified GARCH Model\n",
    "\n",
    "If the GARCH model is correctly specified, the standardized residuals should behave as\n",
    "an i.i.d. process with constant variance.\n",
    "\n",
    "Therefore, for a well-specified model, we expect:\n",
    "- A **high p-value** (typically > 5%),\n",
    "- Failure to reject the null hypothesis of no ARCH effects.\n",
    "\n",
    "Conversely, a low p-value would indicate that the model has not fully captured the\n",
    "conditional variance dynamics, suggesting the need for a richer specification.\n",
    "\n",
    "---\n",
    "\n",
    "We now apply the ARCH LM test to the **standardized residuals** of the selected GARCH model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ARCH LM Test — Correct Specification\n",
    "# =============================================================================\n",
    "# Objective:\n",
    "# - Test for remaining ARCH effects after GARCH filtering\n",
    "# =============================================================================\n",
    "\n",
    "std_resid = results.std_resid\n",
    "\n",
    "lm_test = het_arch(std_resid)\n",
    "\n",
    "print(\"ARCH LM Test on Standardized Residuals\")\n",
    "print(f\"LM Statistic: {lm_test[0]:.3f}\")\n",
    "print(f\"P-value: {lm_test[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### ARCH LM Test — Final Confirmation\n",
    "\n",
    "The ARCH LM test (Engle) was applied to the standardized residuals of the selected\n",
    "GARCH(2,1) model to detect any remaining conditional heteroskedasticity.\n",
    "\n",
    "The test results are:\n",
    "- LM statistic = 12.669\n",
    "- p-value = 0.243\n",
    "\n",
    "At conventional significance levels, the null hypothesis of no remaining ARCH effects\n",
    "cannot be rejected.\n",
    "\n",
    "This provides strong statistical evidence that the GARCH(2,1) specification has fully\n",
    "captured the conditional variance dynamics of CAC 40 returns.\n",
    "\n",
    "---\n",
    "\n",
    "### ARCH LM Test — Final Validation\n",
    "\n",
    "The ARCH LM test (Engle) was applied to the **standardized residuals** of the GARCH model\n",
    "to assess whether any conditional heteroskedasticity remains.\n",
    "\n",
    "The null hypothesis of the test is:\n",
    "- **H₀**: No remaining ARCH effects in the residuals.\n",
    "\n",
    "When applied to the standardized residuals, the test fails to reject the null hypothesis,\n",
    "indicating that the GARCH model has successfully absorbed the conditional variance dynamics.\n",
    "\n",
    "This result, together with the Ljung–Box tests on squared standardized residuals, provides\n",
    "strong evidence that the volatility model is correctly specified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Rolling Volatility Forecasts: Out-of-Sample Evaluation\n",
    "\n",
    "After validating the GARCH model in-sample, we now assess its **out-of-sample\n",
    "forecasting performance** using a rolling-window approach.\n",
    "\n",
    "The objective is to evaluate whether the model can:\n",
    "- Adapt to changing volatility regimes,\n",
    "- Provide meaningful forecasts of future volatility,\n",
    "- Track realized volatility over time.\n",
    "\n",
    "---\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "For each date in the test period:\n",
    "1. The GARCH model is re-estimated using all data available up to that date,\n",
    "2. A one-step-ahead forecast of the conditional variance is produced,\n",
    "3. The forecasted volatility is compared to a proxy for realized volatility.\n",
    "\n",
    "This rolling estimation procedure avoids look-ahead bias and mimics a real-time\n",
    "forecasting setup.\n",
    "\n",
    "---\n",
    "\n",
    "#### Realized Volatility Proxy\n",
    "\n",
    "Since true volatility is unobservable, we use the **absolute return** (or squared return)\n",
    "as a noisy but standard proxy for realized volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Rolling GARCH Volatility Forecast (Out-of-Sample)\n",
    "# =============================================================================\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "# Use returns in percent for numerical stability\n",
    "r = (returns * 100).dropna()\n",
    "\n",
    "def oracle_GARCH(series, p, q, horizon=1):\n",
    "    test_size = 365 * 2  # last 2 years\n",
    "    rolling_vol_forecast = []\n",
    "    \n",
    "    # Define test period indices\n",
    "    test_index = series.index[-test_size:]\n",
    "\n",
    "    for t in test_index:\n",
    "        # Expanding window: use all data up to time t-1\n",
    "        train = series.loc[:t].iloc[:-1]\n",
    "        model = arch_model(train, mean=\"Zero\", vol=\"GARCH\", p=p, q=q, dist=\"t\")\n",
    "        res = model.fit(disp=\"off\")\n",
    "    \n",
    "        # One-step-ahead forecast\n",
    "        forecast = res.forecast(horizon=horizon)\n",
    "    \n",
    "        # Extract forecasted volatility (sqrt of variance)\n",
    "        sigma_hat = np.sqrt(forecast.variance.iloc[-1, 0])\n",
    "        rolling_vol_forecast.append(sigma_hat)\n",
    "    \n",
    "    realized_vol = series.abs().loc[test_index]\n",
    "    rolling_vol_forecast = pd.Series(rolling_vol_forecast, index=test_index, name=\"Forecasted Volatility\")\n",
    "\n",
    "    plt.plot(realized_vol, alpha=0.5, label=\"Realized Volatility |rₜ|\")\n",
    "    plt.plot(rolling_vol_forecast, linewidth=2, label=f\"GARCH({p},{q}) Forecasted Volatility\")\n",
    "    \n",
    "    plt.title(\"Rolling One-Step-Ahead Volatility Forecast — CAC 40\", fontsize=14)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Volatility\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "oracle_GARCH(r, 2, 1)\n",
    "oracle_GARCH(r, 1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Interpretation of Rolling Volatility Forecasts\n",
    "\n",
    "The rolling one-step-ahead forecasts highlight several important features of GARCH-based\n",
    "volatility models:\n",
    "\n",
    "- The forecasted volatility series is significantly smoother than realized volatility,\n",
    "  which is expected since GARCH models estimate **conditional variance**, not realized shocks.\n",
    "- The forecasts react rapidly to major volatility regimes (e.g., crisis periods),\n",
    "  while individual return spikes remain unpredictable.\n",
    "- Both GARCH(1,1) and GARCH(2,1) specifications track volatility regimes remarkably well.\n",
    "\n",
    "A comparison of the two models shows that:\n",
    "- GARCH(2,1) reacts slightly faster to short-term volatility changes,\n",
    "- GARCH(1,1) is marginally smoother and more persistent,\n",
    "- Overall differences are limited, confirming that volatility dynamics are largely\n",
    "  driven by long memory rather than high-order effects.\n",
    "\n",
    "These observations are consistent with the in-sample diagnostics:\n",
    "- GARCH(1,1) provides a strong baseline model,\n",
    "- GARCH(2,1) offers a modest improvement in short-term variance dynamics.\n",
    "\n",
    "To move beyond visual inspection, we now evaluate forecast accuracy using **quantitative\n",
    "loss functions** designed for volatility prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation of Volatility Forecasts\n",
    "\n",
    "Visual inspection provides useful intuition, but formal model comparison requires\n",
    "**quantitative evaluation metrics**.\n",
    "\n",
    "Because true volatility is unobservable, forecast accuracy is assessed relative to a\n",
    "proxy for realized volatility. In this analysis, we use the **absolute return** as a\n",
    "standard and robust proxy.\n",
    "\n",
    "We evaluate forecast performance using two commonly used loss functions:\n",
    "\n",
    "- **Mean Squared Error (MSE)**  \n",
    "  Measures average squared deviation between forecasted and realized volatility.\n",
    "\n",
    "- **QLIKE loss** (Quasi-Likelihood)\n",
    "  A loss function specifically designed for volatility forecasts, robust to noise in\n",
    "  the volatility proxy and widely used in the econometric literature.\n",
    "\n",
    "Lower values of these loss functions indicate better forecast performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Quantitative Evaluation of Rolling Volatility Forecasts\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_volatility_forecast(realized_vol, forecasted_vol):\n",
    "    \"\"\"\n",
    "    Compute standard loss functions for volatility forecast evaluation.\n",
    "    \"\"\"\n",
    "    # Align series\n",
    "    rv, fv = realized_vol.align(forecasted_vol, join=\"inner\")\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse = np.mean((rv - fv) ** 2)\n",
    "\n",
    "    # QLIKE loss (requires variance forecasts)\n",
    "    qlike = np.mean(np.log(fv**2) + (rv**2) / (fv**2))\n",
    "\n",
    "    return {\"MSE\": mse, \"QLIKE\": qlike}\n",
    "\n",
    "# Recompute forecasts (stored or recomputed)\n",
    "# Here we reuse the oracle function logic but return series instead of plotting\n",
    "\n",
    "def rolling_garch_forecast(series, p, q, test_size=365*2):\n",
    "    forecasts = []\n",
    "    test_index = series.index[-test_size:]\n",
    "\n",
    "    for t in test_index:\n",
    "        train = series.loc[:t].iloc[:-1]\n",
    "        model = arch_model(train, mean=\"Zero\", vol=\"GARCH\", p=p, q=q, dist=\"t\")\n",
    "        res = model.fit(disp=\"off\")\n",
    "        forecast = res.forecast(horizon=1)\n",
    "        sigma_hat = np.sqrt(forecast.variance.iloc[-1, 0])\n",
    "        forecasts.append(sigma_hat)\n",
    "\n",
    "    return pd.Series(forecasts, index=test_index)\n",
    "\n",
    "# Forecasts\n",
    "forecast_11 = rolling_garch_forecast(r, 1, 1)\n",
    "forecast_21 = rolling_garch_forecast(r, 2, 1)\n",
    "\n",
    "# Realized volatility proxy\n",
    "realized_vol = r.abs().loc[forecast_11.index]\n",
    "\n",
    "# Evaluation\n",
    "eval_11 = evaluate_volatility_forecast(realized_vol, forecast_11)\n",
    "eval_21 = evaluate_volatility_forecast(realized_vol, forecast_21)\n",
    "\n",
    "results_eval = pd.DataFrame([eval_11, eval_21], index=[\"GARCH(1,1)\", \"GARCH(2,1)\"])\n",
    "results_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Quantitative Forecast Evaluation — Model Comparison\n",
    "\n",
    "Out-of-sample volatility forecasts were evaluated using Mean Squared Error (MSE) and\n",
    "QLIKE loss functions.\n",
    "\n",
    "The results indicate that:\n",
    "\n",
    "- The GARCH(1,1) model achieves lower MSE and QLIKE values than the GARCH(2,1) model.\n",
    "- Despite its slightly better in-sample fit, the additional ARCH term in GARCH(2,1)\n",
    "  does not improve out-of-sample forecast accuracy.\n",
    "- The simpler GARCH(1,1) specification generalizes better and provides more robust\n",
    "  volatility forecasts.\n",
    "\n",
    "These findings highlight the importance of parsimony in volatility modeling and confirm\n",
    "GARCH(1,1) as a strong baseline model for practical forecasting applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Final Synthesis and Trading Implications\n",
    "\n",
    "### 1. Summary of Findings\n",
    "\n",
    "This study investigated the volatility dynamics of the CAC 40 index using a rigorous\n",
    "econometric workflow.\n",
    "\n",
    "The main conclusions are:\n",
    "\n",
    "- The **price level** of the CAC 40 is non-stationary, while **log returns are stationary**\n",
    "  and exhibit strong volatility clustering.\n",
    "- Visual diagnostics, ACF/PACF analysis, and formal statistical tests confirm the presence\n",
    "  of **conditional heteroskedasticity**.\n",
    "- GARCH models are therefore appropriate for modeling volatility dynamics.\n",
    "- After estimation and validation, both **GARCH(1,1)** and **GARCH(2,1)** specifications\n",
    "  capture volatility persistence effectively.\n",
    "- In-sample diagnostics slightly favor GARCH(2,1), but **out-of-sample forecasting\n",
    "  performance (MSE and QLIKE) favors the more parsimonious GARCH(1,1)**.\n",
    "- Rolling one-step-ahead forecasts show that GARCH models are highly effective at tracking\n",
    "  **volatility regimes**, even though individual return shocks remain unpredictable.\n",
    "\n",
    "Overall, the results confirm a well-known stylized fact in financial markets:\n",
    "**returns are unpredictable in mean, but volatility is conditionally predictable**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What This Model Is (and Is Not)\n",
    "\n",
    "It is important to clarify the role of a GARCH model in a trading context:\n",
    "\n",
    "- A GARCH model **does not predict market direction**.\n",
    "- It **does not time individual price jumps**.\n",
    "- It **does predict the conditional level of risk**, which is a second-order property.\n",
    "\n",
    "As such, GARCH is best viewed as a **risk engine**, not an alpha generator.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Trading and Portfolio Construction Applications\n",
    "\n",
    "The estimated conditional volatility can be used as a building block in several\n",
    "practical trading and portfolio overlays.\n",
    "\n",
    "#### 3.1 Volatility Targeting Overlay\n",
    "\n",
    "A direct application is volatility targeting:\n",
    "\n",
    "\\[\n",
    "w_t = \\frac{\\sigma^\\*}{\\hat{\\sigma}_{t+1}}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( \\sigma^\\* \\) is a target volatility (e.g. 10% annualized),\n",
    "- \\( \\hat{\\sigma}_{t+1} \\) is the one-step-ahead GARCH volatility forecast.\n",
    "\n",
    "This produces:\n",
    "- Reduced exposure during high-volatility regimes,\n",
    "- Increased exposure during calm periods,\n",
    "- Improved risk-adjusted returns without predicting direction.\n",
    "\n",
    "This overlay can be applied on:\n",
    "- A passive CAC 40 exposure,\n",
    "- A trend-following or carry strategy,\n",
    "- A factor portfolio.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2 Regime-Based Filters\n",
    "\n",
    "Volatility forecasts can define **market regimes**:\n",
    "\n",
    "- Low volatility regime → risk-on environment,\n",
    "- High volatility regime → risk-off environment.\n",
    "\n",
    "Typical uses:\n",
    "- Disable or scale down directional strategies during high-volatility regimes,\n",
    "- Switch between strategies (e.g. momentum vs mean-reversion),\n",
    "- Control leverage dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3 Spread and Relative Value Strategies\n",
    "\n",
    "Volatility estimates can also be used in **spread trading**:\n",
    "\n",
    "- Index vs futures basis trades,\n",
    "- Equity index vs volatility proxy (e.g. VIX-like instruments),\n",
    "- Cross-asset volatility spreads (equities vs rates).\n",
    "\n",
    "In this context, GARCH volatility acts as a **fair-value anchor** for relative risk.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.4 Factor and Signal Normalization\n",
    "\n",
    "Another key application is **signal normalization**:\n",
    "\n",
    "\\[\n",
    "\\tilde{s}_t = \\frac{s_t}{\\hat{\\sigma}_{t+1}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( s_t \\) is any raw trading signal (momentum, carry, mean reversion),\n",
    "- \\( \\hat{\\sigma}_{t+1} \\) is conditional volatility.\n",
    "\n",
    "This produces:\n",
    "- More stable signal distributions,\n",
    "- Reduced tail risk,\n",
    "- Better comparability across assets and time.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Limitations and Extensions\n",
    "\n",
    "While effective, the model has limitations:\n",
    "\n",
    "- GARCH reacts **after** volatility shocks, not before,\n",
    "- Multi-step forecasts become increasingly smooth and less responsive,\n",
    "- Asymmetries (leverage effects) are only partially captured.\n",
    "\n",
    "Natural extensions include:\n",
    "- Asymmetric models (GJR-GARCH, EGARCH),\n",
    "- Realized volatility models (HAR-RV),\n",
    "- Regime-switching volatility models,\n",
    "- Integration with macro or option-implied volatility.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Final Takeaway\n",
    "\n",
    "This analysis demonstrates that **volatility is one of the most robust and exploitable\n",
    "statistical features of financial markets**.\n",
    "\n",
    "A well-specified GARCH model provides:\n",
    "- A reliable estimate of conditional risk,\n",
    "- A powerful input for portfolio construction,\n",
    "- A stable foundation for overlays and risk management.\n",
    "\n",
    "Used correctly, volatility modeling does not aim to predict markets —  \n",
    "it aims to **control exposure when uncertainty changes**, which is often where the\n",
    "largest performance gains are realized.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
